{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7 classes of documents\n",
    "- 20 docs/class labeled\n",
    "- features/document: 1433 words, an undirected graph where each doc is a node and citations = edges\n",
    "- classify documents\n",
    "\n",
    "Resources\n",
    "---------\n",
    "* Dataset: https://graphsandnetworks.com/the-cora-dataset/\n",
    "* Intro videos, only 20 min total: https://www.youtube.com/watch?v=XRHhtLgpXqg and https://www.youtube.com/watch?v=Zjx25h8DnIo\n",
    "* very good, concise blog post by author of paper: https://tkipf.github.io/graph-convolutional-networks/\n",
    "* paper github repo https://github.com/tkipf/gcn/blob/master/gcn/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, only needed once\n",
    "!mkdir data\n",
    "!curl https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz -o data/cora.tgz\n",
    "!tar -xzf ./data/cora.tgz -C ./data/\n",
    "!rm ./data/cora.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import stellargraph as sg\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from stellargraph.layer.gcn import GraphConvolution, GatherIndices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 23\n",
    "classify using graph convolutional nets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data\n",
    "edgelist = pd.read_csv('./data/cora/cora.cites', sep='\\t', header=None, names=[\"target\", \"source\"])\n",
    "edgelist[\"label\"] = \"cites\"\n",
    "g = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
    "nx.set_node_attributes(g, \"paper\", \"label\")\n",
    "\n",
    "n_features = 1433\n",
    "feature_names = [f\"w_{i}\" for i in range(n_features)]\n",
    "column_names =  feature_names + [\"subject\"]\n",
    "node_data = pd.read_csv('./data/cora/cora.content', sep='\\t', header=None, names=column_names)\n",
    "\n",
    "# Get 20 instances per class for training\n",
    "train_nodes = []\n",
    "classes = node_data.subject.unique()\n",
    "for subject in classes:\n",
    "    train_nodes += node_data[node_data['subject'] == subject].sample(20).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg\n",
    "import tensorflow as tf\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "nodes, edges, targets = node_data, edgelist, node_data[[\"subject\"]]\n",
    "\n",
    "# Use scikit-learn to compute training and test sets\n",
    "train_targets, test_targets = model_selection.train_test_split(targets, train_size=0.5)\n",
    "\n",
    "# convert the raw data into StellarGraph's graph format for faster operations\n",
    "# graph = sg.StellarGraph(nodes, edges)\n",
    "graph = sg.StellarGraph.from_networkx(g, node_features=node_data[feature_names])\n",
    "\n",
    "generator = sg.mapper.FullBatchNodeGenerator(graph, method=\"gcn\")\n",
    "\n",
    "# two layers of GCN, each with hidden dimension 16\n",
    "gcn = sg.layer.GCN(layer_sizes=[16, 16], generator=generator)\n",
    "x_inp, x_out = gcn.in_out_tensors() # create the input and output TensorFlow tensors\n",
    "\n",
    "# use TensorFlow Keras to add a layer to compute the (one-hot) predictions\n",
    "predictions = tf.keras.layers.Dense(units=7, activation=\"softmax\")(x_out)\n",
    "\n",
    "# use the input and output tensors to create a TensorFlow Keras model\n",
    "model = tf.keras.Model(inputs=x_inp, outputs=predictions)\n",
    "\n",
    "# prepare the model for training with the Adam optimiser and an appropriate loss function\n",
    "model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model on the train set\n",
    "model.fit(generator.flow(train_targets.index, train_targets), epochs=5)\n",
    "\n",
    "# check model generalisation on the test set\n",
    "(loss, accuracy) = model.evaluate(generator.flow(test_targets.index, test_targets))\n",
    "print(f\"Test set: loss = {loss}, accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neural Net Hyperparameters\n",
    "learning_rate = 1e-2\n",
    "epochs = 2 # 200\n",
    "hidden_layer_size = 16 # # of units in hidden layer 1\n",
    "dropout = 0.5\n",
    "weight_decay = 5e-4\n",
    "early_stopping = 10\n",
    "max_degree = 3\n",
    "\n",
    "\n",
    "# Create StellarGraph object\n",
    "G = sg.StellarGraph.from_networkx(g, node_features=node_data[feature_names])\n",
    "\n",
    "# Create train and test data\n",
    "train_data = node_data.loc[train_nodes]\n",
    "test_data = node_data.drop(train_nodes)\n",
    "\n",
    "# Create target vectors\n",
    "train_targets = train_data['subject']\n",
    "test_targets = test_data['subject']\n",
    "\n",
    "\n",
    "# Create generator for training data\n",
    "generator = sg.mapper.FullBatchNodeGenerator(G, method=\"gcn\")\n",
    "train_gen = generator.flow(train_data.index, train_targets)\n",
    "\n",
    "# Create GCN model\n",
    "gcn = sg.layer.GCN(\n",
    "    layer_sizes=[hidden_layer_size, len(classes)],\n",
    "    activations=[\"relu\", \"softmax\"],\n",
    "    generator=generator,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "# Create input and output layers\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "# Create model\n",
    "model = tf.keras.Model(inputs=x_inp, outputs=x_out)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    validation_data=None,\n",
    "    shuffle=False,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=early_stopping, restore_best_weights=True)\n",
    "    ],\n",
    ")\n",
    "\n",
    "# # Create test generator\n",
    "# test_gen = generator.flow(test_data.index, test_targets)\n",
    "\n",
    "# # Evaluate model\n",
    "# test_metrics = model.evaluate(test_gen)\n",
    "\n",
    "# print(\"\\nTest Set Metrics:\")\n",
    "# for name, val in zip(model.metrics_names, test_metrics):\n",
    "#     print(\"\\t{}: {:0.4f}\".format(name, val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create node embeddings\n",
    "# embedding_model = tf.keras.Model(inputs=x_inp, outputs=x_out)\n",
    "# emb = embedding_model.predict(generator.flow(G.nodes()))\n",
    "\n",
    "# # Create node embeddings\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Create PCA model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stellargraph.readthedocs.io/en/stable/demos/node-classification/gcn-node-classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing, model_selection\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "epochs = 2 # 200\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "# Load Dataset\n",
    "dataset = sg.datasets.Cora()\n",
    "G, node_subjects = dataset.load()\n",
    "\n",
    "train_subjects, test_subjects = model_selection.train_test_split(\n",
    "    node_subjects, train_size=140, stratify=node_subjects\n",
    ")\n",
    "val_subjects, test_subjects = model_selection.train_test_split(\n",
    "    test_subjects, train_size=500, stratify=test_subjects\n",
    ")\n",
    "\n",
    "# Convert data to one-hot encoding\n",
    "target_encoding = preprocessing.LabelBinarizer()\n",
    "train_targets = target_encoding.fit_transform(train_subjects)\n",
    "val_targets = target_encoding.transform(val_subjects)\n",
    "test_targets = target_encoding.transform(test_subjects)\n",
    "\n",
    "# Create generator for training and validation data\n",
    "generator = FullBatchNodeGenerator(G, method=\"gcn\")\n",
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "validation_gen = generator.flow(val_subjects.index, val_targets)\n",
    "\n",
    "# Create GCN model\n",
    "gcn = GCN(\n",
    "    layer_sizes=[16, 16], activations=[\"relu\", \"relu\"], generator=generator, dropout=dropout\n",
    ")\n",
    "\n",
    "# Create input, output, and prediction layers\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=x_inp, outputs=predictions)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# Track performance\n",
    "es_callback = EarlyStopping(monitor=\"val_acc\", patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_gen,\n",
    "    verbose=2,\n",
    "    shuffle=False,\n",
    "    callbacks=[es_callback],\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Attempt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I need to recreate all the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack Data\n",
    "edgelist = pd.read_csv('./data/cora/cora.cites', sep='\\t', header=None, names=[\"target\", \"source\"])\n",
    "column_names = [f\"w_{i}\" for i in range(1433)] + [\"subject\"]\n",
    "node_data = pd.read_csv('./data/cora/cora.content', sep='\\t', header=None, names=column_names)\n",
    "node_data['subject'] = pd.Categorical(node_data.subject).codes\n",
    "# node_data['subject'] = node_data['subject'].astype('category')\n",
    "\n",
    "# Nodes have weird values like 11573 when there are only 2708 nodes. We map them to [0,2708]\n",
    "node_mapping = dict(zip(node_data.index, np.arange(len(node_data.index))))\n",
    "edgelist['target'] = edgelist['target'].map(node_mapping)\n",
    "edgelist['source'] = edgelist['source'].map(node_mapping)\n",
    "node_data.index = node_data.index.map(node_mapping)\n",
    "\n",
    "# Get 20 instances per class for training\n",
    "train_nodes = []\n",
    "for subject in node_data.subject.unique():\n",
    "    train_nodes += node_data[node_data['subject'] == subject].sample(20).index.to_list()\n",
    "test_nodes = list(set(node_data.index) - set(train_nodes))\n",
    "\n",
    "# Build Graph\n",
    "g = sg.StellarGraph(node_data, edgelist)\n",
    "\n",
    "# Data Preprocessing\n",
    "target_encoding = LabelBinarizer()\n",
    "y_train = target_encoding.fit_transform(node_data.loc[train_nodes]['subject'])[None]\n",
    "y_test = target_encoding.transform(node_data.loc[test_nodes]['subject'])[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Normalized Adjacency Matrix from GCN Paper\n",
    "A      = g.to_adjacency_matrix(weighted=True) # adjacency matrix\n",
    "A_t    = A + np.eye(A.shape[0]) - A.diagonal() # add self-connections to each node\n",
    "D_t    = np.diag(np.power(np.array(A_t.sum(axis=1)), -0.5)[:,0]) # symmetric normalization matrix\n",
    "A_norm = D_t @ A @ D_t # normalized adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_input = node_data.to_numpy()[None]\n",
    "A_input = A_norm[None]\n",
    "train_indices = np.array(train_nodes)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(1, 2708, 1434)]    0           []                               \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (1, 2708, 1434)      0           ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(1, 2708, 2708)]    0           []                               \n",
      "                                                                                                  \n",
      " graph_convolution_12 (GraphCon  (1, 2708, 32)       45920       ['dropout_12[0][0]',             \n",
      " volution)                                                        'input_21[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (1, 2708, 32)        0           ['graph_convolution_12[0][0]']   \n",
      "                                                                                                  \n",
      " graph_convolution_13 (GraphCon  (1, 2708, 32)       1056        ['dropout_13[0][0]',             \n",
      " volution)                                                        'input_21[0][0]']               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(1, None)]          0           []                               \n",
      "                                                                                                  \n",
      " gather_indices_6 (GatherIndice  (1, None, 32)       0           ['graph_convolution_13[0][0]',   \n",
      " s)                                                               'input_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (1, None, 7)         231         ['gather_indices_6[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47,207\n",
      "Trainable params: 47,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialise GCN parameters\n",
    "kernel_initializer=\"glorot_uniform\"\n",
    "bias = True\n",
    "learning_rate=1e-2\n",
    "bias_initializer=\"zeros\"\n",
    "n_layers = 2\n",
    "layer_sizes = [32, 32]\n",
    "dropout = 0.5\n",
    "n_features = features_input.shape[2]\n",
    "n_nodes = features_input.shape[1] # number of classes\n",
    "n_classes = 7\n",
    "early_stopping = 10\n",
    "batch_size = 32\n",
    "epochs = 1 # 200\n",
    "\n",
    "\n",
    "# Create input layer\n",
    "x_features = Input(batch_shape=(1, n_nodes, n_features))\n",
    "x_indices = Input(batch_shape=(1, None), dtype=\"int32\")\n",
    "x_adjacency = Input(batch_shape=(1, n_nodes, n_nodes))\n",
    "x_input  = [x_features, x_indices, x_adjacency]\n",
    "\n",
    "# Build the model\n",
    "x = Dropout(dropout)(x_features)\n",
    "x = GraphConvolution(\n",
    "    32,\n",
    "    activation='relu',\n",
    "    use_bias=True,\n",
    "    kernel_initializer=kernel_initializer,\n",
    "    bias_initializer=bias_initializer\n",
    ")([x, x_adjacency])\n",
    "x = Dropout(dropout)(x)\n",
    "x = GraphConvolution(\n",
    "    32,\n",
    "    activation='relu',\n",
    "    kernel_initializer=kernel_initializer,\n",
    "    bias_initializer=bias_initializer\n",
    ")([x, x_adjacency])\n",
    "x = GatherIndices(batch_dims=1)([x, x_indices])\n",
    "output = Dense(n_classes, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[x_features, x_indices, x_adjacency], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 2s - loss: 1.9146 - acc: 0.3786 - 2s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 1.8712 - acc: 0.3929 - 151ms/epoch - 151ms/step\n",
      "Epoch 3/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 1.7851 - acc: 0.5643 - 142ms/epoch - 142ms/step\n",
      "Epoch 4/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 1.7138 - acc: 0.5929 - 168ms/epoch - 168ms/step\n",
      "Epoch 5/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 1.5897 - acc: 0.6286 - 171ms/epoch - 171ms/step\n",
      "Epoch 6/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 1.4738 - acc: 0.6214 - 139ms/epoch - 139ms/step\n",
      "Epoch 7/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 1.3421 - acc: 0.6643 - 143ms/epoch - 143ms/step\n",
      "Epoch 8/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 1.2002 - acc: 0.7643 - 192ms/epoch - 192ms/step\n",
      "Epoch 9/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 1.0688 - acc: 0.7357 - 184ms/epoch - 184ms/step\n",
      "Epoch 10/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.9374 - acc: 0.8429 - 164ms/epoch - 164ms/step\n",
      "Epoch 11/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.8330 - acc: 0.8143 - 143ms/epoch - 143ms/step\n",
      "Epoch 12/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.7017 - acc: 0.8571 - 153ms/epoch - 153ms/step\n",
      "Epoch 13/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.6206 - acc: 0.8429 - 136ms/epoch - 136ms/step\n",
      "Epoch 14/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.5435 - acc: 0.9000 - 171ms/epoch - 171ms/step\n",
      "Epoch 15/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.4801 - acc: 0.8857 - 149ms/epoch - 149ms/step\n",
      "Epoch 16/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.4066 - acc: 0.9214 - 150ms/epoch - 150ms/step\n",
      "Epoch 17/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.3805 - acc: 0.9214 - 142ms/epoch - 142ms/step\n",
      "Epoch 18/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.2910 - acc: 0.9429 - 141ms/epoch - 141ms/step\n",
      "Epoch 19/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.2732 - acc: 0.9286 - 155ms/epoch - 155ms/step\n",
      "Epoch 20/20\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "1/1 - 0s - loss: 0.2906 - acc: 0.9357 - 156ms/epoch - 156ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "    # loss=tf.losses.binary_crossentropy,\n",
    "    loss=tf.losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "es_callback = EarlyStopping(monitor=\"val_acc\", patience=early_stopping, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    x = [features_input, train_indices, A_input],\n",
    "    y = y_train,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    # validation_data=([x_features, val_subjects.index, x_adjacency], val_targets),\n",
    "    shuffle=False,\n",
    "    callbacks=[es_callback],\n",
    "    # batch_size=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "232",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
